{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "383affba",
   "metadata": {},
   "source": [
    "# Prédire la demande d’un article donné\n",
    "\n",
    "## Objectif\n",
    "\n",
    "Réduire le nombre d'articles unique sans perdre trop de lignes du dataset\n",
    "\n",
    "1. Regroupe certains articles sous la même appelation (ex : galette 4P et galette 6P sous le nom \"galette\")\n",
    "2. Supprimer les lignes des articles très peu vendus (représentant <SEUIL % des ventes totales)\n",
    "\n",
    "---\n",
    "\n",
    "## Résultats de la 1ere idée de réduction (CE N'EST PLUS ALIGNÉ AVEC LE PROJET \"PREDIRE QTÉ INGRÉDIENT DES TOP ARTICLE\")\n",
    "\n",
    "Nombre d'article unique avant modification = 149\n",
    "\n",
    "Nombre d'article unique après Regroupement = 110\n",
    "\n",
    "Nombre d'article unique après Suppression = 60\n",
    "\n",
    "Nombre de lignes avant modifications = 234005\n",
    "\n",
    "Nombre de lignes après suppression des articles très peu vendus = 228107\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b356bf",
   "metadata": {},
   "source": [
    "# 1. Import du dataset original "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "aa32a1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes avant modifications = 234005\n",
      "Nombre d'article unique avant Suppression = 149\n"
     ]
    }
   ],
   "source": [
    "# importer le dataset\n",
    "import pandas as pd\n",
    "data = pd.read_csv('data_tmp/bakery_sales.csv')\n",
    "data.head()\n",
    "\n",
    "print(f\"Nombre de lignes avant modifications = {len(data)}\")\n",
    "print(f\"Nombre d'article unique avant Suppression = {data['article'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc93431",
   "metadata": {},
   "source": [
    "# 2. Exploration et nettoyage  du dataset. \n",
    "\n",
    "## Objectif\n",
    "\n",
    "1. Vérifier quels articles ont été très peu vendus (peu d’occurrences dans le dataset).  \n",
    "2. Afficher la liste complète des articles uniques, triés par ordre alphabétique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d51932",
   "metadata": {},
   "source": [
    "## A. Vérification des colonnes disponibles et des valeurs manquantes\n",
    "CONCLUSION : Rien de manquant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "e89d45f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 234005 entries, 0 to 234004\n",
      "Data columns (total 7 columns):\n",
      " #   Column         Non-Null Count   Dtype  \n",
      "---  ------         --------------   -----  \n",
      " 0   Unnamed: 0     234005 non-null  int64  \n",
      " 1   date           234005 non-null  object \n",
      " 2   time           234005 non-null  object \n",
      " 3   ticket_number  234005 non-null  float64\n",
      " 4   article        234005 non-null  object \n",
      " 5   Quantity       234005 non-null  float64\n",
      " 6   unit_price     234005 non-null  object \n",
      "dtypes: float64(2), int64(1), object(4)\n",
      "memory usage: 12.5+ MB\n",
      "None\n",
      "Nombre de lignes dupliquées : 0\n"
     ]
    }
   ],
   "source": [
    "# Vérifier les colonnes et types de données\n",
    "print(data.info())\n",
    "\n",
    "# Vérifier les valeurs manquantes\n",
    "# AUCUNE\n",
    "\n",
    "# Vérifier s'il y a des doublons exacts\n",
    "nb_duplicated = data.duplicated().sum()\n",
    "print(f\"Nombre de lignes dupliquées : {nb_duplicated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8667211",
   "metadata": {},
   "source": [
    "## A.bis Correction des quantités négatives\n",
    "\n",
    "Il y avait certaines lignes avec des quantités négatives. Après études ça corrrespond à des articles qui ont été remboursés. Nous supprimons donc les lignes négatives et les lignes des achats correspondants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "eaf60176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes avant modifications = 234005\n",
      "Nombre de lignes après modifications = 231423\n"
     ]
    }
   ],
   "source": [
    "# Nombre de lignes avant modifications\n",
    "print(f\"Nombre de lignes avant modifications = {len(data)}\")\n",
    "\n",
    "### On commence par récuperer les ticket_number de tous les tickets négatifs\n",
    "remb_tickets = data.loc[data['Quantity'] < 0, 'ticket_number']\n",
    "\n",
    "### On récupère les ticket_number des tickets qu'ils remboursent et on les join dans un set\n",
    "tickets_a_supprimer = set(remb_tickets).union(remb_tickets - 1)\n",
    "\n",
    "### Enfin on supprimme de la BDD tous les tickets number des tickets à rembourser et les tickets remboursés\n",
    "data = data[~data['ticket_number'].isin(tickets_a_supprimer)]\n",
    "\n",
    "# Nombre de lignes après modifications\n",
    "print(f\"Nombre de lignes après modifications = {len(data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17255345",
   "metadata": {},
   "source": [
    "## B. Compter le nombre de ventes par article, pour supprimer quelques valeurs \"Article\" inutiles\n",
    "\n",
    "CONCLUSION : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a77128",
   "metadata": {},
   "source": [
    "### Afficher la liste des noms d'articles et leur vente total pour faire la selection des modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "03ec7ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'articles uniques avant modification : 148\n",
      "article\n",
      ".                               5\n",
      "12 MACARON                     65\n",
      "ARMORICAIN                      2\n",
      "ARTICLE 295                     1\n",
      "BAGUETTE                    15161\n",
      "BAGUETTE APERO                 59\n",
      "BAGUETTE GRAINE              1489\n",
      "BANETTE                     14909\n",
      "BANETTINE                    2795\n",
      "BOISSON 33CL                 1451\n",
      "BOTTEREAU                      85\n",
      "BOULE 200G                   2667\n",
      "BOULE 400G                   4049\n",
      "BOULE POLKA                   498\n",
      "BRIOCHE                      1642\n",
      "BRIOCHE DE NOEL                19\n",
      "BRIOCHETTE                     46\n",
      "BROWNIES                       36\n",
      "BUCHE 4PERS                    10\n",
      "BUCHE 6PERS                     8\n",
      "BUCHE 8PERS                     1\n",
      "CAFE OU EAU                  1419\n",
      "CAKE                            1\n",
      "CAMPAGNE                     3885\n",
      "CARAMEL NOIX                   69\n",
      "CEREAL BAGUETTE              4906\n",
      "CHAUSSON AUX POMMES          1434\n",
      "CHOCOLAT                       73\n",
      "CHOU CHANTILLY                206\n",
      "COMPLET                      3117\n",
      "COOKIE                       1982\n",
      "COUPE                       20303\n",
      "CROISSANT                   11397\n",
      "CROISSANT AMANDES            1790\n",
      "CRUMBLE                         7\n",
      "CRUMBLECARAMEL OU PISTAE        1\n",
      "DELICETROPICAL                 41\n",
      "DEMI BAGUETTE                1140\n",
      "DEMI PAIN                     163\n",
      "DIVERS BOISSONS                84\n",
      "DIVERS BOULANGERIE            308\n",
      "DIVERS CONFISERIE             482\n",
      "DIVERS PATISSERIE             498\n",
      "DIVERS SANDWICHS              231\n",
      "DIVERS VIENNOISERIE           809\n",
      "DOUCEUR D HIVER                 1\n",
      "ECLAIR                       1983\n",
      "ECLAIR FRAISE PISTACHE         16\n",
      "ENTREMETS                      24\n",
      "FICELLE                      2613\n",
      "FINANCIER                      15\n",
      "FINANCIER X5                 1012\n",
      "FLAN                          817\n",
      "FLAN ABRICOT                  562\n",
      "FONDANT CHOCOLAT              218\n",
      "FORMULE PATE                    2\n",
      "FORMULE PLAT PREPARE           10\n",
      "FORMULE SANDWICH             4052\n",
      "FRAISIER                      353\n",
      "FRAMBOISIER                    64\n",
      "GACHE                         126\n",
      "GAL FRANGIPANE 4P             200\n",
      "GAL FRANGIPANE 6P             122\n",
      "GAL POIRE CHOCO 4P             24\n",
      "GAL POIRE CHOCO 6P             15\n",
      "GAL POMME 4P                   97\n",
      "GAL POMME 6P                   61\n",
      "GALETTE 8 PERS                  2\n",
      "GD FAR BRETON                 154\n",
      "GD KOUIGN AMANN               820\n",
      "GD NANTAIS                     88\n",
      "GD PLATEAU SALE                 2\n",
      "GRAND FAR BRETON             1359\n",
      "GRANDE SUCETTE                193\n",
      "GUERANDAIS                      3\n",
      "KOUIGN AMANN                 1322\n",
      "MACARON                       130\n",
      "MERINGUE                       51\n",
      "MILLES FEUILLES               758\n",
      "MOISSON                      3080\n",
      "NANTAIS                       383\n",
      "NID DE POULE                   12\n",
      "NOIX JAPONAISE                334\n",
      "PAILLE                        103\n",
      "PAIN                         1898\n",
      "PAIN AU CHOCOLAT            10476\n",
      "PAIN AUX RAISINS             1978\n",
      "PAIN BANETTE                 2707\n",
      "PAIN CHOCO AMANDES           1469\n",
      "PAIN DE MIE                    31\n",
      "PAIN GRAINES                    3\n",
      "PAIN NOIR                       1\n",
      "PAIN S/SEL                    119\n",
      "PAIN SUISSE PEPITO             18\n",
      "PALET BRETON                  154\n",
      "PALMIER                         5\n",
      "PARIS BREST                   898\n",
      "PATES                           2\n",
      "PLAQUE TARTE 25P                1\n",
      "PLAT                            8\n",
      "PLAT 7.00                      91\n",
      "PLAT 7.60E                    236\n",
      "PLAT 8.30E                    114\n",
      "PLATPREPARE5,50                 3\n",
      "PLATPREPARE6,00                 1\n",
      "PLATPREPARE6,50                 6\n",
      "PLATPREPARE7,00                11\n",
      "PT NANTAIS                    472\n",
      "PT PLATEAU SALE                 3\n",
      "QUIM BREAD                   1294\n",
      "REDUCTION SUCREES 12            8\n",
      "REDUCTION SUCREES 24            1\n",
      "RELIGIEUSE                     80\n",
      "ROYAL                         172\n",
      "ROYAL 4P                       61\n",
      "ROYAL 6P                       60\n",
      "SABLE F  P                     80\n",
      "SACHET DE CROUTON             107\n",
      "SACHET DE VIENNOISERIE          1\n",
      "SACHET VIENNOISERIE           491\n",
      "SAND JB                         5\n",
      "SAND JB EMMENTAL             1471\n",
      "SANDWICH COMPLET             2168\n",
      "SAVARIN                       337\n",
      "SEIGLE                       1321\n",
      "SPECIAL BREAD                5135\n",
      "SPECIAL BREAD KG              452\n",
      "ST HONORE                      84\n",
      "SUCETTE                       525\n",
      "TARTE FINE                     59\n",
      "TARTE FRAISE 4PER             145\n",
      "TARTE FRAISE 6P               119\n",
      "TARTE FRUITS 4P               350\n",
      "TARTE FRUITS 6P               272\n",
      "TARTELETTE                   2823\n",
      "TARTELETTE CHOC                50\n",
      "TARTELETTE COCKTAIL             1\n",
      "TARTELETTE FRAISE             486\n",
      "THE                             6\n",
      "TRADITIONAL BAGUETTE        66996\n",
      "TRAITEUR                     1287\n",
      "TRIANGLES                      65\n",
      "TROIS CHOCOLAT                  1\n",
      "TROPEZIENNE                   357\n",
      "TROPEZIENNE FRAMBOISE          26\n",
      "TULIPE                          1\n",
      "VIENNOISE                     183\n",
      "VIK BREAD                    3113\n"
     ]
    }
   ],
   "source": [
    "# Nombre total de lignes par article\n",
    "ventes_par_article = data[\"article\"].value_counts()\n",
    "nombre_articles_uniques = len(ventes_par_article)\n",
    "print(f\"Nombre d'articles uniques avant modification : {nombre_articles_uniques}\")\n",
    "\n",
    "#Trier les articles par ordre alphabétique\n",
    "ventes_par_article = ventes_par_article.sort_index()\n",
    "\n",
    "#Afficher tous les articles avec leur nombre de ventes\n",
    "print(ventes_par_article.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a5d908",
   "metadata": {},
   "source": [
    "### Regroupement proposés\n",
    "BAGUETTE APERO                 73 --> BAGUETTE\n",
    "BRIOCHE DE NOEL                19 --> BRIOCHE\n",
    "BRIOCHETTE                     46 --> BRIOCHE\n",
    "DEMI PAIN                     163 --> DEMI PAIN\n",
    "ECLAIR FRAISE PISTACHE         16 --> ECLAIR\n",
    "FINANCIER                      17 --> FINANCIER X5\n",
    "GD FAR BRETON                 161 --> GRAND FAR BRETON\n",
    "PAIN GRAINES                    3 --> PAIN\n",
    "PAIN NOIR                       1 --> PAIN\n",
    "PAIN S/SEL                    131 --> PAIN\n",
    "PAIN SUISSE PEPITO             18 --> PAIN\n",
    "ROYAL 4P                       61 --> ROYAL\n",
    "ROYAL 6P                       62 --> ROYAL\n",
    "SACHET DE VIENNOISERIE          1 --> SACHET VIENNOISERIE\n",
    "SAND JB                         5 --> SAND JB EMMENTAL\n",
    "SPECIAL BREAD KG              460 --> SPECIAL BREAD\n",
    "TARTELETTE CHOC                52 --> TARTELETTE\n",
    "TARTELETTE COCKTAIL             1 --> TARTELETTE\n",
    "TARTELETTE FRAISE             493 --> TARTELETTE\n",
    "TROPEZIENNE FRAMBOISE          26 --> TROPEZIENNE\n",
    "\n",
    "\n",
    "### Création (merger des articles différents dans un nouvel article)\n",
    "GAL FRANGIPANE 4P             203 --> GALETTE FRANGIPANE \n",
    "GAL FRANGIPANE 6P             122 --> GALETTE FRANGIPANE\n",
    "\n",
    "GAL POIRE CHOCO 4P             24 --> GALETTE FRUIT\n",
    "GAL POIRE CHOCO 6P             15 --> GALETTE FRUIT\n",
    "GAL POMME 4P                   97 --> GALETTE FRUIT\n",
    "GAL POMME 6P                   61 --> GALETTE FRUIT\n",
    "GALETTE 8 PERS                  2 --> GALETTE FRUIT\n",
    "\n",
    "PLAT                            8 --> PLAT PREPARE\n",
    "PLAT 6.50E                      2 --> PLAT PREPARE\n",
    "PLAT 7.00                      94 --> PLAT PREPARE\n",
    "PLAT 7.60E                    244 --> PLAT PREPARE\n",
    "PLAT 8.30E                    116 --> PLAT PREPARE\n",
    "PLATPREPARE5,50                 3 --> PLAT PREPARE\n",
    "PLATPREPARE6,00                 1 --> PLAT PREPARE\n",
    "PLATPREPARE6,50                 6 --> PLAT PREPARE\n",
    "PLATPREPARE7,00                11 --> PLAT PREPARE\n",
    "\n",
    "TARTE FINE                     61 --> TARTE FRUITS\n",
    "TARTE FRAISE 4PER             149 --> TARTE FRUITS\n",
    "TARTE FRAISE 6P               125 --> TARTE FRUITS\n",
    "TARTE FRUITS 4P               358 --> TARTE FRUITS\n",
    "TARTE FRUITS 6P               272 --> TARTE FRUITS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229a6ecf",
   "metadata": {},
   "source": [
    "### On trie les articles du moins vendu au plus vendu, puis on prend les premiers tant que la somme cumulée reste ≤ 1 % des ventes totales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "eb387460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seuil pour 1% des ventes totales : 3600 ventes\n",
      "Articles les moins vendus dont les ventes cumulées représentent au plus 1% des ventes totales :\n",
      "article\n",
      "SACHET DE VIENNOISERIE        1.0\n",
      "PAIN NOIR                     1.0\n",
      "REDUCTION SUCREES 24          1.0\n",
      "ARTICLE 295                   1.0\n",
      "CAKE                          1.0\n",
      "DOUCEUR D HIVER               1.0\n",
      "TROIS CHOCOLAT                1.0\n",
      "PLATPREPARE6,00               1.0\n",
      "BUCHE 8PERS                   2.0\n",
      "FORMULE PATE                  2.0\n",
      "GD PLATEAU SALE               2.0\n",
      "GALETTE 8 PERS                2.0\n",
      "TARTELETTE COCKTAIL           2.0\n",
      "PATES                         2.0\n",
      "TULIPE                        2.0\n",
      "PLAQUE TARTE 25P              2.0\n",
      "PLATPREPARE5,50               3.0\n",
      "GUERANDAIS                    3.0\n",
      "PAIN GRAINES                  3.0\n",
      "ARMORICAIN                    3.0\n",
      "CRUMBLECARAMEL OU PISTAE      3.0\n",
      "PT PLATEAU SALE               4.0\n",
      "SAND JB                       5.0\n",
      "PALMIER                       6.0\n",
      "THE                           7.0\n",
      ".                             7.0\n",
      "PLATPREPARE6,50               8.0\n",
      "REDUCTION SUCREES 12          8.0\n",
      "BUCHE 6PERS                   8.0\n",
      "PLAT                          9.0\n",
      "CRUMBLE                       9.0\n",
      "FORMULE PLAT PREPARE         11.0\n",
      "BUCHE 4PERS                  11.0\n",
      "GAL POIRE CHOCO 6P           15.0\n",
      "PLATPREPARE7,00              17.0\n",
      "BRIOCHE DE NOEL              19.0\n",
      "FINANCIER                    20.0\n",
      "PAIN SUISSE PEPITO           21.0\n",
      "ECLAIR FRAISE PISTACHE       22.0\n",
      "GAL POIRE CHOCO 4P           24.0\n",
      "TROPEZIENNE FRAMBOISE        33.0\n",
      "NID DE POULE                 35.0\n",
      "ENTREMETS                    38.0\n",
      "PAIN DE MIE                  40.0\n",
      "BROWNIES                     45.0\n",
      "TARTELETTE CHOC              60.0\n",
      "DELICETROPICAL               61.0\n",
      "GAL POMME 6P                 62.0\n",
      "BAGUETTE APERO               62.0\n",
      "TARTE FINE                   65.0\n",
      "MERINGUE                     65.0\n",
      "ROYAL 6P                     66.0\n",
      "12 MACARON                   70.0\n",
      "ROYAL 4P                     78.0\n",
      "TRIANGLES                    81.0\n",
      "CHOCOLAT                     82.0\n",
      "BRIOCHETTE                   86.0\n",
      "DIVERS BOISSONS              87.0\n",
      "GAL POMME 4P                 99.0\n",
      "CARAMEL NOIX                100.0\n",
      "FRAMBOISIER                 107.0\n",
      "GD NANTAIS                  109.0\n",
      "ST HONORE                   110.0\n",
      "PLAT 7.00                   111.0\n",
      "RELIGIEUSE                  119.0\n",
      "TARTE FRAISE 6P             120.0\n",
      "SACHET DE CROUTON           121.0\n",
      "SABLE F  P                  128.0\n",
      "GAL FRANGIPANE 6P           128.0\n",
      "GACHE                       129.0\n",
      "PAILLE                      130.0\n",
      "PAIN S/SEL                  140.0\n",
      "PLAT 8.30E                  146.0\n",
      "DEMI PAIN                   163.0\n",
      "TARTE FRAISE 4PER           165.0\n",
      "\n",
      "Total cumulé de ces articles : 3511.0 ventes\n",
      "Ce qui représente 0.975% des ventes totales.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Comptage des ventes par article (somme des Quantities)\n",
    "ventes_par_article = data.groupby(\"article\")[\"Quantity\"].sum()\n",
    "\n",
    "# Connaitre les articles dont le cumul représente au plus 1% des ventes totales\n",
    "ventes_totales = ventes_par_article.sum()\n",
    "seuil_1pct_ventes = ventes_totales * 0.01\n",
    "\n",
    "print(f\"Seuil pour 1% des ventes totales : {seuil_1pct_ventes:.0f} ventes\")\n",
    "\n",
    "# Trier les articles du moins vendu au plus vendu (par unités vendues)\n",
    "ventes_tries = ventes_par_article.sort_values(ascending=True)\n",
    "\n",
    "# Somme cumulée des ventes en partant du moins vendu\n",
    "cumul_ventes = ventes_tries.cumsum()\n",
    "\n",
    "# Garder les articles tant que le cumul reste <= 1% des ventes totales\n",
    "articles_sous_1pct = ventes_tries[cumul_ventes <= seuil_1pct_ventes]\n",
    "\n",
    "print(\"Articles les moins vendus dont les ventes cumulées représentent au plus 1% des ventes totales :\")\n",
    "print(articles_sous_1pct.to_string())\n",
    "\n",
    "print(f\"\\nTotal cumulé de ces articles : {articles_sous_1pct.sum()} ventes\")\n",
    "print(f\"Ce qui représente {articles_sous_1pct.sum() / ventes_totales * 100:.3f}% des ventes totales.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c648f324",
   "metadata": {},
   "source": [
    "# 3. Reduction du dataset (LE SEUIL DES TOP ARTICLE EST FIXÉ ICI)\n",
    "\n",
    "Objectif : Ne garder que les lignes à propos de TOP ARTICLES (i.e. ceux qui represente + de 75-85% des ventes totales)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f322e9",
   "metadata": {},
   "source": [
    "## A. Regroupement de certain nom d'articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966094e4",
   "metadata": {},
   "source": [
    "### V1 CODE MORT (mais problème de variation des prix entre articles)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "f5f2ef79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 1) Normaliser (majuscule + strip)\\ndata[\"article_norm\"] = data[\"article\"].astype(str).str.upper().str.strip()\\n\\n# 2) Dictionnaire mapping exact (source -> cible) basé sur ta liste\\nmapping_exact = {\\n    # Regroupement proposés\\n    \"BRIOCHE DE NOEL\": \"BRIOCHE\",\\n    \"BRIOCHETTE\": \"BRIOCHE\",\\n    \"DEMI PAIN\": \"DEMI PAIN\",\\n    \"ECLAIR FRAISE PISTACHE\": \"ECLAIR\",\\n    \"FINANCIER\": \"FINANCIER X5\",\\n    \"GD FAR BRETON\": \"GRAND FAR BRETON\",\\n    \"GD KOUIGN AMANN\": \"KOUIGN AMANN\",\\n    \"GD NANTAIS\": \"NANTAIS\",\\n    \"PT NANTAIS\": \"NANTAIS\",\\n    \"GRANDE SUCETTE\": \"SUCETTE\",\\n    \"PAIN GRAINES\": \"PAIN\",\\n    \"PAIN NOIR\": \"PAIN\",\\n    \"PAIN S/SEL\": \"PAIN\",\\n    \"PAIN SUISSE PEPITO\": \"PAIN\",\\n    \"ROYAL 4P\": \"ROYAL\",\\n    \"ROYAL 6P\": \"ROYAL\",\\n    \"SACHET VIENNOISERIE\": \"SACHET DE VIENNOISERIE\",\\n    \"SAND JB\": \"SAND JB EMMENTAL\",\\n    \"SPECIAL BREAD KG\": \"SPECIAL BREAD\",\\n    \"TARTELETTE CHOC\": \"TARTELETTE\",\\n    \"TARTELETTE COCKTAIL\": \"TARTELETTE\",\\n    \"TARTELETTE FRAISE\": \"TARTELETTE\",\\n    \"TROPEZIENNE FRAMBOISE\": \"TROPEZIENNE\",\\n    # Création / mergers (on met aussi en exact pour la sécurité)\\n    \"BOULE 200G\": \"BOULE GLACE\",\\n    \"BOULE 400G\": \"BOULE GLACE\",\\n    \"BUCHE 4PERS\": \"BUCHE\",\\n    \"BUCHE 6PERS\": \"BUCHE\",\\n    \"BUCHE 8PERS\": \"BUCHE\",\\n    \"GAL FRANGIPANE 4P\": \"GALETTE FRANGIPANE\",\\n    \"GAL FRANGIPANE 6P\": \"GALETTE FRANGIPANE\",\\n    \"GAL POIRE CHOCO 4P\": \"GALETTE FRUIT\",\\n    \"GAL POIRE CHOCO 6P\": \"GALETTE FRUIT\",\\n    \"GAL POMME 4P\": \"GALETTE FRUIT\",\\n    \"GAL POMME 6P\": \"GALETTE FRUIT\",\\n    \"GALETTE 8 PERS\": \"GALETTE FRUIT\",\\n    \"PLAT\": \"PLAT PREPARE\",\\n    \"PLAT 6.50E\": \"PLAT PREPARE\",\\n    \"PLAT 7.00\": \"PLAT PREPARE\",\\n    \"PLAT 7.60E\": \"PLAT PREPARE\",\\n    \"PLAT 8.30E\": \"PLAT PREPARE\",\\n    \"PLATPREPARE5,50\": \"PLAT PREPARE\",\\n    \"PLATPREPARE6,00\": \"PLAT PREPARE\",\\n    \"PLATPREPARE6,50\": \"PLAT PREPARE\",\\n    \"PLATPREPARE7,00\": \"PLAT PREPARE\",\\n    \"TARTE FINE\": \"TARTE FRUITS\",\\n    \"TARTE FRAISE 4PER\": \"TARTE FRUITS\",\\n    \"TARTE FRAISE 6P\": \"TARTE FRUITS\",\\n    \"TARTE FRUITS 4P\": \"TARTE FRUITS\",\\n    \"TARTE FRUITS 6P\": \"TARTE FRUITS\",\\n}\\n\\n\\n# 4) Appliquer mapping exact\\ndata[\"article_reduced\"] = data[\"article_norm\"].map(mapping_exact)\\n\\n\\n\\n# 6) Remplir le reste par l\\'article normalisé (ou choisir de garder l\\'original si préféré)\\ndata[\"article_reduced\"] = data[\"article_reduced\"].fillna(data[\"article_norm\"])\\n\\n# 7) Vérifications rapides\\nprint(\"Nombre d\\'articles uniques avant modification :\", data[\"article\"].nunique())\\nprint(\"Nombre d\\'articles uniques après regroupement :\", data[\"article_reduced\"].nunique())\\n\\n'"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# 1) Normaliser (majuscule + strip)\n",
    "data[\"article_norm\"] = data[\"article\"].astype(str).str.upper().str.strip()\n",
    "\n",
    "# 2) Dictionnaire mapping exact (source -> cible) basé sur ta liste\n",
    "mapping_exact = {\n",
    "    # Regroupement proposés\n",
    "    \"BRIOCHE DE NOEL\": \"BRIOCHE\",\n",
    "    \"BRIOCHETTE\": \"BRIOCHE\",\n",
    "    \"DEMI PAIN\": \"DEMI PAIN\",\n",
    "    \"ECLAIR FRAISE PISTACHE\": \"ECLAIR\",\n",
    "    \"FINANCIER\": \"FINANCIER X5\",\n",
    "    \"GD FAR BRETON\": \"GRAND FAR BRETON\",\n",
    "    \"GD KOUIGN AMANN\": \"KOUIGN AMANN\",\n",
    "    \"GD NANTAIS\": \"NANTAIS\",\n",
    "    \"PT NANTAIS\": \"NANTAIS\",\n",
    "    \"GRANDE SUCETTE\": \"SUCETTE\",\n",
    "    \"PAIN GRAINES\": \"PAIN\",\n",
    "    \"PAIN NOIR\": \"PAIN\",\n",
    "    \"PAIN S/SEL\": \"PAIN\",\n",
    "    \"PAIN SUISSE PEPITO\": \"PAIN\",\n",
    "    \"ROYAL 4P\": \"ROYAL\",\n",
    "    \"ROYAL 6P\": \"ROYAL\",\n",
    "    \"SACHET VIENNOISERIE\": \"SACHET DE VIENNOISERIE\",\n",
    "    \"SAND JB\": \"SAND JB EMMENTAL\",\n",
    "    \"SPECIAL BREAD KG\": \"SPECIAL BREAD\",\n",
    "    \"TARTELETTE CHOC\": \"TARTELETTE\",\n",
    "    \"TARTELETTE COCKTAIL\": \"TARTELETTE\",\n",
    "    \"TARTELETTE FRAISE\": \"TARTELETTE\",\n",
    "    \"TROPEZIENNE FRAMBOISE\": \"TROPEZIENNE\",\n",
    "    # Création / mergers (on met aussi en exact pour la sécurité)\n",
    "    \"BOULE 200G\": \"BOULE GLACE\",\n",
    "    \"BOULE 400G\": \"BOULE GLACE\",\n",
    "    \"BUCHE 4PERS\": \"BUCHE\",\n",
    "    \"BUCHE 6PERS\": \"BUCHE\",\n",
    "    \"BUCHE 8PERS\": \"BUCHE\",\n",
    "    \"GAL FRANGIPANE 4P\": \"GALETTE FRANGIPANE\",\n",
    "    \"GAL FRANGIPANE 6P\": \"GALETTE FRANGIPANE\",\n",
    "    \"GAL POIRE CHOCO 4P\": \"GALETTE FRUIT\",\n",
    "    \"GAL POIRE CHOCO 6P\": \"GALETTE FRUIT\",\n",
    "    \"GAL POMME 4P\": \"GALETTE FRUIT\",\n",
    "    \"GAL POMME 6P\": \"GALETTE FRUIT\",\n",
    "    \"GALETTE 8 PERS\": \"GALETTE FRUIT\",\n",
    "    \"PLAT\": \"PLAT PREPARE\",\n",
    "    \"PLAT 6.50E\": \"PLAT PREPARE\",\n",
    "    \"PLAT 7.00\": \"PLAT PREPARE\",\n",
    "    \"PLAT 7.60E\": \"PLAT PREPARE\",\n",
    "    \"PLAT 8.30E\": \"PLAT PREPARE\",\n",
    "    \"PLATPREPARE5,50\": \"PLAT PREPARE\",\n",
    "    \"PLATPREPARE6,00\": \"PLAT PREPARE\",\n",
    "    \"PLATPREPARE6,50\": \"PLAT PREPARE\",\n",
    "    \"PLATPREPARE7,00\": \"PLAT PREPARE\",\n",
    "    \"TARTE FINE\": \"TARTE FRUITS\",\n",
    "    \"TARTE FRAISE 4PER\": \"TARTE FRUITS\",\n",
    "    \"TARTE FRAISE 6P\": \"TARTE FRUITS\",\n",
    "    \"TARTE FRUITS 4P\": \"TARTE FRUITS\",\n",
    "    \"TARTE FRUITS 6P\": \"TARTE FRUITS\",\n",
    "}\n",
    "\n",
    "\n",
    "# 4) Appliquer mapping exact\n",
    "data[\"article_reduced\"] = data[\"article_norm\"].map(mapping_exact)\n",
    "\n",
    "\n",
    "\n",
    "# 6) Remplir le reste par l'article normalisé (ou choisir de garder l'original si préféré)\n",
    "data[\"article_reduced\"] = data[\"article_reduced\"].fillna(data[\"article_norm\"])\n",
    "\n",
    "# 7) Vérifications rapides\n",
    "print(\"Nombre d'articles uniques avant modification :\", data[\"article\"].nunique())\n",
    "print(\"Nombre d'articles uniques après regroupement :\", data[\"article_reduced\"].nunique())\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "38feb16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# créer un csv avec la liste des articles uniques dans l'ordre alphabétique\n",
    "# unique_articles_initial = data[\"article\"].unique()\n",
    "# unique_articles_initial.sort()\n",
    "# pd.DataFrame(unique_articles_initial, columns=[\"article\"]).to_csv(\"data_tmp/unique_articles_initial.csv\", index=False)\n",
    "\n",
    "# unique_articles = data[\"article_reduced\"].unique()\n",
    "# unique_articles.sort()\n",
    "# pd.DataFrame(unique_articles, columns=[\"article_reduced\"]).to_csv(\"data_tmp/unique_articles_reduced1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "f2058f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Remettre la colonne article à la place de article_reduced et supprimer les colonnes intermédiaires\\ndata[\"article\"] = data[\"article_reduced\"]\\ndata = data.drop(columns=[\"article_norm\", \"article_reduced\"])\\n'"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Remettre la colonne article à la place de article_reduced et supprimer les colonnes intermédiaires\n",
    "data[\"article\"] = data[\"article_reduced\"]\n",
    "data = data.drop(columns=[\"article_norm\", \"article_reduced\"])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b459e2",
   "metadata": {},
   "source": [
    "### V2 Avec seulement des articles à prix constants (on mettra la prix de l'article le + représenté )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "c6470772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'articles uniques avant modification : 148\n",
      "Nombre d'articles uniques après regroupement : 131\n"
     ]
    }
   ],
   "source": [
    "# 1) Normaliser (majuscule + strip)\n",
    "data[\"article_norm\"] = data[\"article\"].astype(str).str.upper().str.strip()\n",
    "\n",
    "# 2) Dictionnaire mapping exact (source -> cible) basé sur ta liste\n",
    "mapping_exact = {\n",
    "    # Regroupement proposés\n",
    "    \"FINANCIER X5\": \"FINANCIER\",\n",
    "    \"GRANDE SUCETTE\": \"SUCETTE\",\n",
    "    \"SACHET VIENNOISERIE\": \"SACHET DE VIENNOISERIE\",\n",
    "    \"SAND JB EMMENTAL\": \"SAND JB\",\n",
    "    # Création / mergers (on met aussi en exact pour la sécurité)\n",
    "    \"BUCHE 4PERS\": \"BUCHE\",\n",
    "    \"BUCHE 6PERS\": \"BUCHE\",\n",
    "    \"BUCHE 8PERS\": \"BUCHE\",\n",
    "    \"GAL FRANGIPANE 4P\": \"GALETTE FRANGIPANE\",\n",
    "    \"GAL FRANGIPANE 6P\": \"GALETTE FRANGIPANE\",\n",
    "    \"GAL POIRE CHOCO 4P\": \"GALETTE FRUIT\",\n",
    "    \"GAL POIRE CHOCO 6P\": \"GALETTE FRUIT\",\n",
    "    \"GAL POMME 4P\": \"GALETTE FRUIT\",\n",
    "    \"GAL POMME 6P\": \"GALETTE FRUIT\",\n",
    "    \"GALETTE 8 PERS\": \"GALETTE FRUIT\",\n",
    "    \"PLAT 6.50E\": \"PLAT PREPARE\",\n",
    "    \"PLAT 7.00\": \"PLAT PREPARE\",\n",
    "    \"PLAT 7.60E\": \"PLAT PREPARE\",\n",
    "    \"PLAT 8.30E\": \"PLAT PREPARE\",\n",
    "    \"PLATPREPARE5,50\": \"PLAT PREPARE\",\n",
    "    \"PLATPREPARE6,00\": \"PLAT PREPARE\",\n",
    "    \"PLATPREPARE6,50\": \"PLAT PREPARE\",\n",
    "    \"PLATPREPARE7,00\": \"PLAT PREPARE\",\n",
    "}\n",
    "\n",
    "\n",
    "# 4) Appliquer mapping exact\n",
    "data[\"article_reduced\"] = data[\"article_norm\"].map(mapping_exact)\n",
    "\n",
    "\n",
    "\n",
    "# 6) Remplir le reste par l'article normalisé (ou choisir de garder l'original si préféré)\n",
    "data[\"article_reduced\"] = data[\"article_reduced\"].fillna(data[\"article_norm\"])\n",
    "\n",
    "# 7) Vérifications rapides\n",
    "print(\"Nombre d'articles uniques avant modification :\", data[\"article\"].nunique())\n",
    "print(\"Nombre d'articles uniques après regroupement :\", data[\"article_reduced\"].nunique())\n",
    "\n",
    "# Remettre la colonne article à la place de article_reduced et supprimer les colonnes intermédiaires\n",
    "data[\"article\"] = data[\"article_reduced\"]\n",
    "data = data.drop(columns=[\"article_norm\", \"article_reduced\"])\n",
    "\n",
    "# MODIFICATION PRIX\n",
    "# créer une colonne numérique pour calculs (ex: \"0,90 €\" -> 0.90)\n",
    "data['unit_price_num'] = (\n",
    "    data['unit_price'].astype(str)\n",
    "        .str.replace(r'[^0-9,.-]', '', regex=True)   # garde chiffres, virgule, point, -\n",
    "        .str.replace(',', '.', regex=False)          # virgule -> point\n",
    ")\n",
    "data['unit_price_num'] = pd.to_numeric(data['unit_price_num'], errors='coerce')\n",
    "\n",
    "# 1) Unifier par la moyenne pour certains articles (format string \"0,90 €\")\n",
    "to_average = ['BUCHE', 'GALETTE FRANGIPANE', 'GALETTE FRUIT', 'PLAT PREPARE','FINANCIER',\n",
    "               'SUCETTE', 'SAND JB', 'SACHET DE VIENNOISERIE']\n",
    "\n",
    "def format_euro(x):\n",
    "    return f\"{x:.2f}\".replace('.', ',') + \" €\"\n",
    "\n",
    "for art in to_average:\n",
    "    vals = data.loc[data['article'] == art, 'unit_price_num'].dropna()\n",
    "    if vals.empty:\n",
    "        continue\n",
    "    mean_val = vals.mean()\n",
    "    data.loc[data['article'] == art, 'unit_price'] = format_euro(mean_val)\n",
    "\n",
    "\n",
    "\n",
    "# supprimer colonne intermédiaire numérique\n",
    "data.drop(columns=['unit_price_num'], inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a5f2b6",
   "metadata": {},
   "source": [
    "## A.bis suppression de l'article \"coupe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "59c7eda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Supprimer artcle 'coupe'\n",
    "data = data[data['article'] != 'COUPE']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142c97f6",
   "metadata": {},
   "source": [
    "## B. Suppression des articles dont la vente cumulée est < à SEUIL %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "02731e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seuil pour 25.0% des ventes totales : 84130 ventes\n",
      "Articles les moins vendus dont les ventes cumulées représentent au plus 25.0% des ventes totales :\n",
      "article\n",
      "CAKE                           1.0\n",
      "DOUCEUR D HIVER                1.0\n",
      "ARTICLE 295                    1.0\n",
      "TROIS CHOCOLAT                 1.0\n",
      "PAIN NOIR                      1.0\n",
      "REDUCTION SUCREES 24           1.0\n",
      "FORMULE PATE                   2.0\n",
      "GD PLATEAU SALE                2.0\n",
      "PATES                          2.0\n",
      "PLAQUE TARTE 25P               2.0\n",
      "TARTELETTE COCKTAIL            2.0\n",
      "TULIPE                         2.0\n",
      "PAIN GRAINES                   3.0\n",
      "ARMORICAIN                     3.0\n",
      "CRUMBLECARAMEL OU PISTAE       3.0\n",
      "GUERANDAIS                     3.0\n",
      "PT PLATEAU SALE                4.0\n",
      "PALMIER                        6.0\n",
      "THE                            7.0\n",
      ".                              7.0\n",
      "REDUCTION SUCREES 12           8.0\n",
      "PLAT                           9.0\n",
      "CRUMBLE                        9.0\n",
      "FORMULE PLAT PREPARE          11.0\n",
      "BRIOCHE DE NOEL               19.0\n",
      "PAIN SUISSE PEPITO            21.0\n",
      "BUCHE                         21.0\n",
      "ECLAIR FRAISE PISTACHE        22.0\n",
      "TROPEZIENNE FRAMBOISE         33.0\n",
      "NID DE POULE                  35.0\n",
      "ENTREMETS                     38.0\n",
      "PAIN DE MIE                   40.0\n",
      "BROWNIES                      45.0\n",
      "TARTELETTE CHOC               60.0\n",
      "DELICETROPICAL                61.0\n",
      "BAGUETTE APERO                62.0\n",
      "MERINGUE                      65.0\n",
      "TARTE FINE                    65.0\n",
      "ROYAL 6P                      66.0\n",
      "12 MACARON                    70.0\n",
      "ROYAL 4P                      78.0\n",
      "TRIANGLES                     81.0\n",
      "CHOCOLAT                      82.0\n",
      "BRIOCHETTE                    86.0\n",
      "DIVERS BOISSONS               87.0\n",
      "CARAMEL NOIX                 100.0\n",
      "FRAMBOISIER                  107.0\n",
      "GD NANTAIS                   109.0\n",
      "ST HONORE                    110.0\n",
      "RELIGIEUSE                   119.0\n",
      "TARTE FRAISE 6P              120.0\n",
      "SACHET DE CROUTON            121.0\n",
      "SABLE F  P                   128.0\n",
      "GACHE                        129.0\n",
      "PAILLE                       130.0\n",
      "PAIN S/SEL                   140.0\n",
      "DEMI PAIN                    163.0\n",
      "TARTE FRAISE 4PER            165.0\n",
      "GD FAR BRETON                170.0\n",
      "GALETTE FRUIT                202.0\n",
      "VIENNOISE                    218.0\n",
      "DIVERS SANDWICHS             233.0\n",
      "PALET BRETON                 265.0\n",
      "CHOU CHANTILLY               271.0\n",
      "TARTE FRUITS 6P              283.0\n",
      "ROYAL                        287.0\n",
      "FONDANT CHOCOLAT             318.0\n",
      "DIVERS BOULANGERIE           319.0\n",
      "GALETTE FRANGIPANE           340.0\n",
      "TARTE FRUITS 4P              380.0\n",
      "BOTTEREAU                    389.0\n",
      "MACARON                      447.0\n",
      "NOIX JAPONAISE               447.0\n",
      "SPECIAL BREAD KG             459.0\n",
      "SAVARIN                      467.0\n",
      "DIVERS CONFISERIE            503.0\n",
      "TROPEZIENNE                  508.0\n",
      "SACHET DE VIENNOISERIE       510.0\n",
      "BOULE POLKA                  531.0\n",
      "DIVERS PATISSERIE            536.0\n",
      "NANTAIS                      592.0\n",
      "FRAISIER                     593.0\n",
      "PLAT PREPARE                 610.0\n",
      "PT NANTAIS                   754.0\n",
      "FLAN ABRICOT                 791.0\n",
      "TARTELETTE FRAISE            804.0\n",
      "DIVERS VIENNOISERIE          832.0\n",
      "GD KOUIGN AMANN             1009.0\n",
      "MILLES FEUILLES             1050.0\n",
      "FLAN                        1077.0\n",
      "DEMI BAGUETTE               1143.0\n",
      "PARIS BREST                 1267.0\n",
      "FINANCIER                   1281.0\n",
      "SUCETTE                     1405.0\n",
      "SEIGLE                      1423.0\n",
      "TRAITEUR                    1454.0\n",
      "GRAND FAR BRETON            1485.0\n",
      "BRIOCHE                     1703.0\n",
      "SAND JB                     1798.0\n",
      "BAGUETTE GRAINE             1862.0\n",
      "BOISSON 33CL                1872.0\n",
      "PAIN CHOCO AMANDES          1908.0\n",
      "CHAUSSON AUX POMMES         1936.0\n",
      "CAFE OU EAU                 1945.0\n",
      "QUIM BREAD                  2181.0\n",
      "CROISSANT AMANDES           2349.0\n",
      "KOUIGN AMANN                2383.0\n",
      "PAIN                        2461.0\n",
      "PAIN AUX RAISINS            2733.0\n",
      "SANDWICH COMPLET            2890.0\n",
      "PAIN BANETTE                3021.0\n",
      "BOULE 200G                  3080.0\n",
      "BANETTINE                   3092.0\n",
      "MOISSON                     3363.0\n",
      "FICELLE                     3405.0\n",
      "COMPLET                     3534.0\n",
      "VIK BREAD                   3619.0\n",
      "ECLAIR                      3655.0\n",
      "\n",
      "Total cumulé de ces articles : 83312.0 ventes\n",
      "Ce qui représente 24.757% des ventes totales.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Comptage des ventes par article (somme des Quantities)\n",
    "ventes_par_article = data.groupby(\"article\")[\"Quantity\"].sum()\n",
    "\n",
    "seuil = 0.25\n",
    "\n",
    "# Connaitre les articles dont le cumul représente au plus 2% des ventes totales\n",
    "ventes_totales = ventes_par_article.sum()\n",
    "seuil_pct_ventes = ventes_totales * seuil \n",
    "\n",
    "print(f\"Seuil pour {seuil*100}% des ventes totales : {seuil_pct_ventes:.0f} ventes\")\n",
    "\n",
    "# Trier les articles du moins vendu au plus vendu (par unités vendues)\n",
    "ventes_tries = ventes_par_article.sort_values(ascending=True)\n",
    "\n",
    "# Somme cumulée des ventes en partant du moins vendu\n",
    "cumul_ventes = ventes_tries.cumsum()\n",
    "\n",
    "# Garder les articles tant que le cumul reste <= seuil% des ventes totales\n",
    "articles_sous_pct = ventes_tries[cumul_ventes <= seuil_pct_ventes]\n",
    "\n",
    "print(f\"Articles les moins vendus dont les ventes cumulées représentent au plus {seuil*100}% des ventes totales :\")\n",
    "print(articles_sous_pct.to_string())\n",
    "\n",
    "print(f\"\\nTotal cumulé de ces articles : {articles_sous_pct.sum()} ventes\")\n",
    "print(f\"Ce qui représente {articles_sous_pct.sum() / ventes_totales * 100:.3f}% des ventes totales.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "0adb0e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes après suppression des articles très peu vendus : 145771\n",
      "Nombre d'article unique après Suppression = 12\n"
     ]
    }
   ],
   "source": [
    "# suppression des lignes avec des articles très peu vendus\n",
    "articles_a_supprimer = articles_sous_pct.index.tolist()\n",
    "data = data[~data[\"article\"].isin(articles_a_supprimer)]\n",
    "print(f\"Nombre de lignes après suppression des articles très peu vendus : {len(data)}\")\n",
    "\n",
    "print(f\"Nombre d'article unique après Suppression = {data['article'].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f770bf",
   "metadata": {},
   "source": [
    "La liste des 'top article' sous le seuil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "07aefdde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liste des 'top articles' après suppression des articles très peu vendus :\n",
      "['BAGUETTE', 'PAIN AU CHOCOLAT', 'TRADITIONAL BAGUETTE', 'CROISSANT', 'BANETTE', 'SPECIAL BREAD', 'BOULE 400G', 'CAMPAGNE', 'CEREAL BAGUETTE', 'COOKIE', 'FORMULE SANDWICH', 'TARTELETTE']\n"
     ]
    }
   ],
   "source": [
    "liste_top_articles = data[\"article\"].unique().tolist()\n",
    "print(\"Liste des 'top articles' après suppression des articles très peu vendus :\")\n",
    "print(liste_top_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0349d4f",
   "metadata": {},
   "source": [
    "## C. Data to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "c7652be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('data_tmp/bakery_sales_reduce.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30b6b4f",
   "metadata": {},
   "source": [
    "# ANNEXE CLARA (Regroupement en Famille d'articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "451b7df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Families definition\n",
    "boulangerie = [\n",
    "    'BAGUETTE', 'PAIN', 'TRADITIONAL BAGUETTE', 'BANETTE', 'BANETTINE',\n",
    "    'SPECIAL BREAD', 'COUPE', 'BOULE 200G', 'BOULE 400G', 'CAMPAGNE',\n",
    "    'MOISSON', 'CEREAL BAGUETTE', 'SEIGLE', 'COMPLET', 'FICELLE',\n",
    "    'VIK BREAD', 'PAIN BANETTE', 'QUIM BREAD', 'BOULE POLKA',\n",
    "    'DEMI BAGUETTE', 'BAGUETTE GRAINE', 'DEMI PAIN'\n",
    "]\n",
    "\n",
    "viennoiserie = [\n",
    "    'CROISSANT', 'PAIN AU CHOCOLAT', 'PAIN AUX RAISINS', 'CROISSANT AMANDES',\n",
    "    'PAIN CHOCO AMANDES', 'SACHET VIENNOISERIE', 'KOUIGN AMANN',\n",
    "    'GD KOUIGN AMANN', 'NANTAIS', 'BOTTEREAU', 'VIENNOISE',\n",
    "    'DIVERS VIENNOISERIE'\n",
    "]\n",
    "\n",
    "patisserie = [\n",
    "    'TARTELETTE', 'FLAN', 'FLAN ABRICOT', 'Paris Brest'.upper(), 'MILLES FEUILLES',\n",
    "    'ECLAIR', 'CHOU CHANTILLY', 'SAVARIN', 'ROYAL', 'TARTE FRUITS',\n",
    "    'TROPEZIENNE', 'FRAISIER', 'NOIX JAPONAISE', 'FONDANT CHOCOLAT',\n",
    "    'GALETTE FRANGIPANE', 'GALETTE FRUIT', 'GRAND FAR BRETON'\n",
    "]\n",
    "\n",
    "confiserie = [\n",
    "    'COOKIE', 'FINANCIER X5', 'SUCETTE', 'GRANDE SUCETTE', 'PALET BRETON',\n",
    "    'MACARON', 'DIVERS CONFISERIE'\n",
    "]\n",
    "\n",
    "snacking = [\n",
    "    'SAND JB EMMENTAL', 'SANDWICH COMPLET', 'DIVERS SANDWICHS',\n",
    "    'FORMULE SANDWICH', 'BOISSON 33CL', 'CAFE OU EAU', 'TRAITEUR',\n",
    "    'PLAT PREPARE'\n",
    "]\n",
    "\n",
    "# mapping\n",
    "def get_famille(article):\n",
    "    if article in boulangerie:\n",
    "        return 'Boulangerie'\n",
    "    elif article in viennoiserie:\n",
    "        return 'Viennoiserie'\n",
    "    elif article in patisserie:\n",
    "        return 'Pâtisserie'\n",
    "    elif article in confiserie:\n",
    "        return 'Confiserie'\n",
    "    elif article in snacking:\n",
    "        return 'Snacking'\n",
    "    else:\n",
    "        return 'Autre'\n",
    "data['Famille_traditionnelle'] = data['article'].apply(get_famille)\n",
    "len(data['Famille_traditionnelle'].unique())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13 (vscode)",
   "language": "python",
   "name": "vscode-py313"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
